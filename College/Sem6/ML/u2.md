# Supervised Learning - Parametric Methods
## Classification
### Vapnik Chervovenkis Dimension
The maximum number of points (instances) that can be `shattered` by H (hypothesis) is denoted by `VC(H)` and measures the capacity of H

### Probably Approximate Correct Learning
<!-- TODO: Revisit -->

### Noise
- Recording error
- Teacher's noise (mislabelling)
- Additional attributes

## Regression
- $$g(x) = W_1\ x_1\ +\ W_2\ x_2\ +...+\ W_d\ x_d\ +\ W_0$$
- $$E(g|x) = \frac{1}{N}\sum {[r^t - g(x^t)]}^2$$
- $$W_1 = \frac{\sum{x^t r^t} - N\ \overline{r}\ \overline{x}}{\sum{(x^t)}^2 - N\ \overline{x}^{\ 2}}$$
- $$W_0 = \overline{r} - W_1\ \overline{x}$$
- $$E_{RSE} = \frac{\sum{[r^t - g(x^t | \theta)]}}{\sum{(r^t - \overline{r})}^{\ 2}}$$
- $$R^2 = 1- E_{RSE}$$
## Triple Trade Off
- Complexity of hypothesis
- Amount of training data
- Generalisation error

## Decisions for Making a model
- Model we use
- Loss Function
- Optmization Technique

## Bayes Theorem
$$P(A_1|B) = \frac{P(A_1)\ P(B|A_1)}{P(A_1)\ P(B|A_1) + P(A_2)\ P(B|A_2)}$$

## Parametric Classification
- Discriminant function: $$g_i(x) = P(x|C_i)\ P(C_i) \rightarrow\ log{P(x|C_i)}\ +\ log{P(C_i)}$$
- $$P(x|C_i) = \frac{1}{\sqrt{2\pi}\sigma_i}exp[-\frac{(x - \mu_i)^2}{2\sigma_i^2}]$$

## Model Selection Procedure
- Cross Validation
- Regularization
- AIC (Akaike's Information Criterion) & BIC (Bayesian Information Criterion)
- Structural Risk Management
- Minimum Description Length
- Bayesian Model Selection